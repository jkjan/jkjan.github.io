---
title : PyTorch  
categories : [Deep Learning]  
date : 2020-03-21  
---  

본 문서는 [PyTorch](https://pytorch.org/docs/stable/torch.html) 의 공식 문서를 번역한 것입니다.  
번역이 오히려 혼란을 가져올만한 단어거나  
굳이 번역할 필요가 없는 경우, 또는 적절한 한국어 번역이 없는 단어는 번역하지 않았습니다.    


# Torch  

Torch 패키지는 다차원의 텐서를 위한 자료 구조와  
이들을 정의하는 수학적 연산 처리들을 가지고 있습니다.  
추가적으로 Torch 패키지는 텐서들의 효율적인 직렬화와  
임의의 자료형, 그리고 그 외의 다른 유용한 기능들을 제공합니다.  

Torch 패키지는 CUDA와 대응하여 여러분의 텐서 연산을  
compute capability 3.0 이상의 NVIDIA GPU에서 실행할 수 있습니다.  
[NVIDIA GPU별 compute capability](https://developer.nvidia.com/cuda-gpus#collapse4)  

## 텐서  

`torch.is_tensor(obj)`  
  obj가 PyTorch 의 텐서라면 True를 반환합니다.  
  `매개변수` 
     obj(객체) - 시험해볼 객체  

`torch.is_storage(obj)`  
  obj가 PyTorch 저장소 객체라면 True를 반환합니다.  
  `매개변수`  
    obj(객체) - 시험해볼 객체  
  
`torch.is_floating_point(input) -> (bool)`  
  input의 자료형이 `torch.float64`, `torch.float32`와 `torch.float16` 중 하나와 같은  
  부동 소수점의 자료형이라면 True를 반환합니다.  
  `매개변수`  
    input(Tensor) - 시험해볼 PyTorch 텐서  
    
`torch.set_default_dtype(d)`  
   기본 부동 소수점 dtype을 d로 설정합니다.   
   이 자료형은 `torch.tensor()`의 자료형 추론에서 기본 부동 소수점 자료형으로 쓰입니다.  
   기본 부동 소수점 자료형은 `torch.float32`로 초기화되어있습니다.  
   `매개변수`  
     d(torch.dtype) - 기본으로 할 부동 소수점 dtype  
     
     예:  
     
     ```console
     >>> torch.tensor([1.2, 3]).dtype             # 부동 소수점의 초기 설정
     torch.float32  
     torch.float32  
     >>> torch.set_default_dtype(torch.float64)   # 새로운 부동 소수점 텐서  
     >>> torch.tensor([1.2, 3).dtype  
     torch.float64  
     ```
     
`torch.get_default_dtype() -> torch.dtype`  
  현재의 기본 부동 소수점 torch.dtype을 얻습니다.  
  
  예:  
  
  ```console  
  >>> torch.get_default_dtype()    # 부동 소수점의 초기 설정은 torch.float32입니다.  
  torch.float32  
  >>> torch.set_default_dtype(torch.float64)  
  >>> torch.get_default_dtype()    # 기본값이 torch.float64 로 바뀌었습니다.  
  torch.float64  
  >>> torch.set_default_tensor_type(torch.FloatTensor)  # 텐서 유형을 바꾸는 것도 됩니다.  
  this  
  >>> torch.get_default_dtype()    # torch.FloatTensor 의 dtype인 torch.float32 로 바뀌었습니다.   
  torch.float32  
  ```
  
`torch.set_default_tensor_type(t)`  
  기본 `torch.Tensor`의 유형을 유형 `t`의 부동 소수점 텐서로 바꿉니다.  
  이 자료형은 `torch.tensor()`의 자료형 추론의 기본 부동 소수점 자료형으로 쓰입니다.  
  
  부동 소수점 텐서 자료형의 기본값은 `torch.FloatTensor`입니다.  
  `매개변수`  
    t(파이썬:자료형 혹은 문자열) - 부동 소수점 텐서의 자료형이나 그 이름  
    
  예:  
  
  ```console  
  >>> torch.tensor([1.2, 3]).dtype   # 부동 소수점의 기본값은 torch.float32입니다.  
  torch.float32  
  >>> torch.set_default_tensor_type(torch.DoubleTensor)  
  >>> torch.tensor([1.2, 3]).dtype   # 새 부동 소수점 텐서  
  torch.float64  
  ```

`torch.numel(input) -> int`  
  `input` 텐서의 총 원소 수를 반환합니다.  
  `매개변수`  
     input(Tensor) - 입력 텐서  
   
   예:  
   
   ```console  
   >>> a = torch.randn(1, 2, 3, 4, 5)  
   >>> torch.numel(a)  
   120  
   >>> a = torch.zeros(4, 4)  
   >>> torch.numel(a)  
   16
   ```
   
`torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None,  
profile=None, sci_mode=None)`  
  출력 시 옵션을 설정합니다. 항목은 NumPy에서 그대로 가져옵니다.  
  
  `매개변수`  
    * precision - 부동 소수점 출력에서 정확도 자리 수 (기본값 = 4).  
    
    * threshold - 전체 출력하기보다는 요약할 총 배열 원소의 수 (기본값 = 1000).   
    
    * edgeitems - 각 차원의 시작과 끝에서의 배열 항목 수 (기본값 = 3).  
    
    * linewidth - 개행 문자를 넣기 위해 정하는 줄마다의 글자 수 (기본값 = 80).  
      한계치를 넘은 행렬은 이 매개변수를 무시합니다.  
      
    * profile - 예쁜 출력을 위한 정상적인 기본값. 상기 옵션들을 무시할 수 있습니다. (기본값, short, full)  
    
    * sci_mode - 과학 표시 (scientific notation)을 켜거나(True) 끕니다(False).   
    None(기본값)이 특정되었다면, 그 값은 `_Formatter`로 결정됩니다.  
    
`torch.set_flush_denormal(mode) -> bool`  
  CPU에서의 denormal 부동 소수점 기능을 끕니다.  
  
  여러분의 시스템이 denormal number의 flushing 을 지원한다면 True를 반환하고  
  flush denormal mode를 성공적으로 설정합니다.  
  `set_flush_denormal()`은 SSE3를 지원하는 x86 아키텍쳐에서만 지원합니다. 
  `매개변수`  
    mode(bool) - flush denormal mode를 켤 것인지 끌 것인지를 제어합니다.  
    
  예:  
  
  ```console  
  >>> torch.set_flush_denormal(True)  
  True  
  >>> torch.tensor([1e-323], dtype=torch.float64)  
  tensor([0.], dtype=torch.float64)  
  >>> torch.set_flush_denormal(False)  
  True  
  >>> torch.tensor([1e-323], dtype=torch.float64)  
  tensor(9.88131e-324 *  
         [1.0000], dtype=torch.float64)  
  ```       


생성 작업  

랜덤하게 샘플링하는 생성 작업은 Random sampling 아래 리스트돼있으며  
이는 `torch_rand()`, `torch.rand_like()`, `torch.randn()`, `torch.randn_like()`, `torch.randint()`,  
`torch.randint_like()`, `torch.randperm()` 등이 있습니다.  
`torch.empty()`와 In-place random sampling 메소드들을 활용하여  
넓은 범위의 분포로 샘플된 값의 `torch.Tensor`를 생성할 수 있습니다.  

`torch.tensor(data, dtype=None, device=None, requires_glad_False, pin_memory=False) -> Tensor`  
  `data`로 텐서를 형성합니다.  
  
  **주의** : 
  `torch.tensor`는 항상 `data`를 복사합니다. 여러분이 텐서 `data`를 가지고 있고 복사를 원치 않는다면  
  `torch.Tensor.requires_glad_()`나 `torch.Tensor.detach()`를 사용하십시오. NumPy의 `ndarray`를 가지고 있고  
  이 역시 복사를 원치 않으신다면 `torch.as_tensor()`를 사용하십시오.  
  
  **주의** :  
  데이터를 텐서 x라고 할 때, `torch.tensor()`는 무엇이 전해지든지 그 데이터를 읽어내고,  
  leaf variable을 형성합니다.  
  그러므로 `torch.tensor(x)`는 `x.clone().detach()`와 동일한 표현이며  
  `torch.tensor(x, requires_glad=True)`는 `x.clone().detach().requires_glad_(True)`와 같은 표현입니다.  
  `clone()`과 `detach()`를 이용한 동일 표현이 추천됩니다.  
  
  `매개변수`  
    * data (배열과 같은) - 텐서의 초기 데이터입니다. 리스트일 수도, 튜플일 수도, NumPy의 `ndarray`일 수도,  
    scalar 또는 다른 자료형일 수 있습니다.  
    
    * dtype (`torch.dtype`, 선택적) - 반환되는 텐서의 원하는 자료형입니다. 기본값으로 `None`이라면 `data`로부터 자료형을 추론합니다.  
    
    * device (`torch.device`, 선택적) - 반환되는 텐서의 원하는 device입니다. 기본값으로 `None`이라면  
    기본 텐서 타입(`torch.set_default_tensor_type()`을 확인하십시오)의 현재 device가 됩니다.  
    
    `device`는 CPU 텐서 자료형의 CPU이고, CUDA 텐서 자료형의 현재 CUDA 디바이스입니다.  
    
    * requires_grad(bool, 선택적) - 반환된 텐서에서 자동기울기(autograd)가 기록될 지를 정합니다. 기본값은 False입니다.  
    
    * pin_memory (bool, 선택적) - 설정할 시, 반환되는 텐서는 pinned memory에 할당됩니다. CPU 텐서에서만 작동합니다. 기본값은 False입니다.  
    
    
    예:  
    
    ```
    >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])
    tensor([[ 0.1000,  1.2000],
        [ 2.2000,  3.1000],
        [ 4.9000,  5.2000]])

    >>> torch.tensor([0, 1])  # 데이터에서 자료형을 추론
    tensor([ 0,  1])

    >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],
                     dtype=torch.float64,
                     device=torch.device('cuda:0'))  # torch.cuda.DoubleTensor 형을 생성
    tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')

    >>> torch.tensor(3.14159)  # 스칼라를 생성 (0차원 텐서)  
    tensor(3.1416)

    >>> torch.tensor([])  size(0, )의 빈 텐서를 생성
    tensor([])
    ```
    
`torch.sparse_coo_tensor(indices, values, size=None, dtype=None, device=None, requires_grad=False) -> Tensor`  
  전달된 `values`의 `indices`에 있는 0이 아닌 원소로 COO 형식의 희소 행렬을 형성합니다. (COO는 Coordinate의 약자)  
  희소 텐서는 맞지 않을 수도 있는데,   
  이 경우엔 인덱스들에 겹치는 좌표가 있고   
  해당 인덱스의 값은 그 겹치는 값들의 합이 됩니다:[torch.sparse](https://pytorch.org/docs/stable/sparse.html)  
  `매개변수`  
    * indices (배열과 같은) - 텐서의 초기 데이터입니다.  
    리스트, 튜플, NumPy의 `ndarray`, 스칼라, 그 외의 자료형이 올 수 있습니다.  
    이는 내부적으로 `torch.LongTensor`로 cast 됩니다. 인덱스들은 행렬에서 0이 아닌 값들의 좌표이고  
    따라서 첫번째 차원이 텐서 차원의 수이고 두번째 차원이 그 0이 아닌 값들의 수인 2차원 행렬이어야 합니다.  
    
    * values (배열과 같은) - 텐서의 초기 값입니다. 리스트, 튜플, NumPy의 `ndarray`, 스칼라, 그 외의 자료형이 올 수 있습니다.  
    
    * size (리스트, 튜플이나 `torch.Size`, 선택적) - 희소 텐서의 크기입니다. 제공되지 않는다면   
    0이 아닌 모든 원소를 담을만한 크기로 설정됩니다.  
    
    * dtype (`torch.dtype`, 선택적) - 반환되는 텐서의 원하는 자료형입니다. None이라면 기본값으로 `values`에서 추론됩니다.  
    
    * device  (`torch.device`, 선택적) - 반환되는  텐서의 원하는 device입니다. None이라면 기본값으로  
    기본 텐서 타입(`torch.set_default_tensor_type()`을 확인하십시오)의 현재 device가 됩니다.  
    
    * requires_grad(bool, 선택적) - 반환된 텐서에서 자동기울기(autograd)가 기록될 지를 정합니다. 기본값은 False입니다.  
    
    예:  
    
    ```console  
    >>> i = torch.tensor([[0, 1, 1],  
                          [2, 0, 2]])  
    >>> v = torch.tensor([3, 4, 5], dtype=torch.float32)  
    >>> torch.sparse_coo_tensor(i, v, [2, 4])  
    tensor(indices=tensor([[0, 1, 1],
                           [2, 0, 2]]),  
           values=tensor([3., 4., 5.]),  
           size=(2, 4), nnz=3, layout=torch.sparse_coo)  
           
    >>> torch.sparse_coo_tensor(i, v)  #  shape 추론  
    tensor(indices=tensor([[0, 1, 1],  
                           [2, 0, 2]]),  
           value=tensor([3., 4., 5.]), 
           size=(2, 3), nnz=3, layout=torch.sparse_coo)  
           
    >>> torch.sparse_coo_tensor(i, v, [2, 4],  
                                dtype=torch.float64, 
                                device=torch.device('cuda:0'))  
    tensor(indices=tensor([[0, 1, 1],
                           [2, 0, 2]]),  
           values=tensor([3., 4., 5.]),  
           device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,  
           layout=torch.sparse_coo)  
           
     # 다음과 같은 방식으로 빈 희소 텐서를 생성합니다:  
     # 1. sparse_dim + dense_dim = len(SparseTensor.shape)  
     # 2. SparseTensor._indices().shape = (sparse_dim, nnz)  
     # 3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])  
     #
     # 예를 들어, nnz = 0, dense_dim = 0, sparse_dim = 1 (즉 인덱스는 (1, 0) 모양의 2차원 텐서) 를 생성하려면  
     >>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])  
     tensor(indices=tensor([], size=(1, 0)),  
            values=tensor([], size=(0,)),  
            size=(1,), nnz=0, layout=torch.sparse_coo)  
            
     # 그리고 nnz = 0, dense_dim = 1, sparse_dim = 1 인 희소 텐서를 생성하려면  
     >>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])  
     tensor(indices=tensor([], size=(1, 0)),  
            values=tensor([], size=(0, 2)),  
            size=(1, 2), nnz=0, layout=torch.sparse_coo)  
            ```  
            
  **역주** : indices은 모양(shape)이 ('차원 수', '0이 아닌 원수의 수') 가 되는 2차원 행렬이어야 한다.  
  예시의 행렬에서 indices의 shape는 (2, 3)으로  
  희소 행렬은 2차원이므로 2행, 0이 아닌 값(value)는 3개이므로 3열이 되는 것이다.  
  indices 행렬은 column 방향으로 읽는다.  
  이 예에서는 `[0, 2]`, `[1, 0]`, `[1, 2]`에 0이 아닌 값 3, 4, 5이 들어가고 나머지가 다 0인 희소 행렬을  
  COO 형식으로 저장한다.  
  이를 Dense 행렬로 다시 펼치면 (`torch.to_dense()`)    
  
  ```console  
  >>> tt.to_dense()
  tensor([[0., 0., 3.],
          [4., 0., 5.]])
         ```
  와 같은 결과가 나온다.  
            
`torch.as_tensor(data, dtype=None, device=None) -> Tensor`  
  데이터를 torch.Tensor로 변환합니다. 만일 데이터가 같은 dtype에 같은 device의 텐서라면  
  복사는 일어나지 않고, 새로운 텐서가 반환됩니다.  
  데이터 텐서가 `requires_grad=True`라면 연산 그래프도 또한 반환됩니다.  
  비슷하게, 데이터가 같은 dtype의 `ndarray`이고 device의 cpu도 같다면 복사는 일어나지 않습니다.  
  
  `매개변수`  
  * data  
  * dtype  
  * device    
  위의 각 상응하는 매개변수 설명과 동일합니다.  
  
  예:  
  
  ```console  
  >>> a = numpy.array([1, 2, 3])  
  >>> t = torch.as_tensor(a)  
  >>> t  
  tensor([1, 2, 3])  
  >>> t[0] = -1  
  >>> a  
  array([-1, 2, 3])  
  
  >>> a = numpy.array([1, 2, 3])  
  >>> t = torch.as_tensor(a, device=torch.device('cuda'))  
  >>> t  
  tensor([1, 2, 3])  
  >>> t[0]= -1  
  >>> a  
  array([1, 2, 3])  
  ```
  
`torch.as_strided(input, size, stride, storage_offset=0) -> Tensor`  
  이미 존재하는 torch.Tensor인 `input`을 `size`, `stride`, `storage_offset`로 특정한 뷰를 생성합니다.  
  
  **주의** :  
  생성된 텐서의 한 개 이상의 원소가 한 memory location을 참조합니다.  
  결과적으로, in-place 동작(특히 vectorized 된 것들)은 잘못된 결과를 낼 수 있습니다.  
  텐서에 써야 한다면 이들을 먼저 복사하십시오.  
  
  텐서의 view를 반환하는 PyTorch의 많은 함수들은 내부적으로 이 함수를 구현합니다.  
  `torch.Tensor.expand()` 같은 이런 함수들은 읽기에 쉬워서 사용이 추천됩니다.  
  
  `매개변수`  
  * input (텐서) - 입력할 텐서 
  * size (튜플이거나 파이썬 정수) - 출력 텐서의 모양  
  * stride (튜플이거나 파이썬 정수) - 출력 텐서의 stride  
  * storage_offset (파이썬 정수, 선택적) - 출력 텐서의 underlying storage의 offset  
  
  예:  
  
  ```console
  >>> x = torch.randn(3, 3)  
  >>> x  
  tensor([[ 0.9039,  0.6291,  1.0795],
          [ 0.1586,  2.1939, -0.4900],
          [-0.1909, -0.7503,  1.9355]])
  >>> t = torch.as_strided(x, (2, 2), (1, 2))
  >>> t
  tensor([[0.9039, 1.0795],
          [0.6291, 0.1586]])
  >>> t = torch.as_strided(x, (2, 2), (1, 2), 1)
  tensor([[0.6291, 0.1586],
          [1.0795, 2.1939]])  
  ```  
  
