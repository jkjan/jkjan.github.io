---
date : 2020-02-08
title : Natural Language Processing
categories : [Natural Language Processing]
---

## 자연어 처리

2011년 애플의 시리를 시작으로 점점 사용자의 음성을 인식해서 요청을 처리하는 프로그램이 늘고 있다.  
삼성엔 빅스비, 구글엔 구글 어시스턴트가 있고  
다른 스마트폰 제조사마다 각각의 소프트웨어를 탑재하고 있다.  
과연 스마트폰, 그러니까 컴퓨터는 우리의 말을 어떻게 인식하는 것일까.

소리, 음파를 문자를 어찌저찌 바꿨다고 치자.  
그래서 "음악 좀 틀어줘" 라는 문자열을 생성했는데, 컴퓨터는 이걸 어떻게 이해하고 음악을 틀어주는 것일까?  

"음악 좀 틀어줘" 라는 것은 그저 15바이트에 문자열에 지나지 않는다.  
컴퓨터는 그저 '음', '악', '좀', '틀', '어', '줘'를 2바이트의 공간에, 나머지 공백은 각 1바이트에 공간에 적용한다.  
정렬 알고리즘이 복잡한 점에서도 알 수 있듯이 컴퓨터는 폰 노이만 구조로, 메모리는 선형(linear) 이다.  
동시에 시각적으로 이미지를 처리하는 사람과 달리 컴퓨터는 한 번에 하나의 자료 밖에 보지 못한다.  

컴퓨터가 이 문자열을 동시에 볼 수 있더라도 달라지는 것은 없다.  
언어는 의미를 가지지 않으면 쓸모없는 텍스트 덩어리에 불과하다.  
즉 자연어 처리라는 것은 컴퓨터가 글자, 단어, 문장의 의미를 알게 하는 것을 의미한다. 

컴퓨터란 지능이 없기 때문에 우리가 사용하는 언어의 실제 의미를 가르치는 것은 절대로 불가능하다.  
컴퓨터에게 이미지를 학습시킬 때 이미지의 픽셀에 대한 정보를 행렬(벡터)로 수치화시켰듯이,  
자연스럽게 언어도 수치화시킬 수 있는 방법이 없을까 하고 생각이 닿게 된다.  

이렇게 단어를 벡터로 수치화하는 것을 '단어 임베딩'이라 한다.  

## 단어 임베딩

어떤 물건을 물건들 사이에 고정해 끼워넣는 것을 'embed' 라고 한다.  

Word embedding is the collective name for a set of language modeling  
and feature learning techniques in natural language processing (NLP)  
where words or phrases from the vocabulary are mapped to vectors of real numbers.   

단어 임베딩이란 자연어 처리에서 언어 모델링과 특징 학습 기술에 사용되는 것을 통틀어 이르는 말이다.  
자연어 처리에서는 어휘에서 나온 단어나 구가 실수의 벡터로 매핑된다.  

우리가 쓰는 '나비', '컴퓨터', '커피'나, '커피를 먹는다.', '노래를 듣는다.' 등의 단어나 구가  
벡터로 표현될 수 있다는 말이다.  

도대체 어떻게 '나비'를 수치화할 수 있다는 말인가?  
세상의 모든 언어를 분류해서 동물, 곤충 쪽의 성분은 강하게, 기계, 컴퓨터 쪽의 성분을 약하게 한다는 것인가?  

세상에는 언어가 너무나도 많고 그 언어에도 단어가 너무나도 많아서 그 단어들을 일일히 정하는 것은 불가능하다.  
그리고 컴퓨터를 위한 자연어-벡터 사전을 만든다 해도, 컴퓨터는 언어를 배웠다고 할 수 없다.  
영한사전을 통째로 외웠다 해서 영어를 잘하는 것이 아니듯 말이다.  

언어는, 작게 말해 단어는 그 단어가 얼마나, 어떻게, 어떤 단어와 같이 쓰이는지에 따라 의미가 있다.  
컴퓨터에게 실제 그 단어의 의미를 가르치진 못해도 단어를 수치적으로 나타낼 수 있는 것이다.  
우리의 언어는 사전 속에서 있는 것이 아니라 실제 사용되면서의미가 발현되기 때문에,  
실제 용례를 통계적인 자료로 만들어 학습을 시킨다.  

### 어떤 단어가 많이 쓰였는가

예를 들어 자연어 처리를 공부하는 책에서는 '자연어'나 '인공지능'이란 단어가 많이 쓰일 것이다.  
이 정보로 그 책은 '자연어', '인공지능'이 주제일 것이라고 예측을 할 수가 있고,  
다른 책들과 비교했을 때도 그 차이가 날 것이므로  
이를 통해서 책의 종류를 나눌 수 있을 것이다.   

### 단어가 어떤 순서로 쓰였는가

'커피' 다음에는 '마신다'가 높은 확률로 같이 쓰인다. '쏟았다', '먹었다' 도 올 수 있겠지만  
'마신다'의 확률이 훨씬 더 높을 것이다.   
이 확률을 알려주면 컴퓨터도 커피가 뭔지 마신다가 뭐하는 건진 몰라도  
적어도 '커피를 ???' 의 '???'에 '마신다' 가 올 것이라는 것을 예측할 수 있게 된다.  

### 어떤 단어가 같이 쓰였는가

순서도 순서이지만, 단어는 항상 붙어다니는 친구들이 있다.  
바늘 가는데 실 가듯이, 인공지능에 대해 설명하는 문서에는 항상 딥러닝이나 머신러닝이라는 단어가 나타날 것이다.  
은행 관련 문서에는 돈이나 투자, 이자 같은 단어가 나타날 것이다.  
이런 식으로 단어를 학습하면 항상 같은 순서로 나타나진 않아도 관련 있는 단어를 찾아내기 쉬울 것이다.  


이렇듯 단어를 임베딩하는 방법에는 크게 세 가지가 있다.  

1. 단어를 세기 (Bag of words, TF-IDF...)    
2. 단어의 순서 고려 (n-gram...)  
3. 같이 쓰인 단어 고려 (word2vec, fasttext...)    
