---
date : 2020-02-08
title : Natural Language Processing
categories : [Natural Language Processing]
---

## 자연어 처리

2011년 애플의 시리를 시작으로 점점 사용자의 음성을 인식해서 요청을 처리하는 프로그램이 늘고 있다.  
삼성엔 빅스비, 구글엔 구글 어시스턴트가 있고  
다른 스마트폰 제조사마다 각각의 소프트웨어를 탑재하고 있다.  
과연 스마트폰, 그러니까 컴퓨터는 우리의 말을 어떻게 인식하는 것일까.

소리, 음파를 문자를 어찌저찌 바꿨다고 치자.  
그래서 "음악 좀 틀어줘" 라는 문자열을 생성했는데, 컴퓨터는 이걸 어떻게 이해하고 음악을 틀어주는 것일까?  

"음악 좀 틀어줘" 라는 것은 그저 15바이트에 문자열에 지나지 않는다.  
컴퓨터는 그저 '음', '악', '좀', '틀', '어', '줘'를 2바이트의 공간에, 나머지 공백은 각 1바이트에 공간에 적용한다.  
정렬 알고리즘이 복잡한 점에서도 알 수 있듯이 컴퓨터는 폰 노이만 구조로, 메모리는 선형(linear) 이다.  
동시에 시각적으로 이미지를 처리하는 사람과 달리 컴퓨터는 한 번에 하나의 자료 밖에 보지 못한다.  

컴퓨터가 이 문자열을 동시에 볼 수 있더라도 달라지는 것은 없다.  
언어는 의미를 가지지 않으면 쓸모없는 텍스트 덩어리에 불과하다.  
즉 자연어 처리라는 것은 컴퓨터가 글자, 단어, 문장의 의미를 알게 하는 것을 의미한다. 

컴퓨터란 지능이 없기 때문에 우리가 사용하는 언어의 실제 의미를 가르치는 것은 절대로 불가능하다.  
컴퓨터에게 이미지를 학습시킬 때 이미지의 픽셀에 대한 정보를 행렬(벡터)로 수치화시켰듯이,  
자연스럽게 언어도 수치화시킬 수 있는 방법이 없을까 하고 생각이 닿게 된다.  

이렇게 단어를 벡터로 수치화하는 것을 '단어 임베딩'이라 한다.  

## 단어 임베딩

어떤 물건을 물건들 사이에 고정해 끼워넣는 것을 'embed' 라고 한다.  

Word embedding is the collective name for a set of language modeling  
and feature learning techniques in natural language processing (NLP)  
where words or phrases from the vocabulary are mapped to vectors of real numbers.   

단어 임베딩이란 자연어 처리에서 언어 모델링과 특징 학습 기술에 사용되는 것을 통틀어 이르는 말이다.  
자연어 처리에서는 어휘에서 나온 단어나 구가 실수의 벡터로 매핑된다.  

우리가 쓰는 '나비', '컴퓨터', '커피'나, '커피를 먹는다.', '노래를 듣는다.' 등의 단어나 구가  
벡터로 표현될 수 있다는 말이다.  

도대체 어떻게 '나비'를 수치화할 수 있다는 말인가?  
세상의 모든 언어를 분류해서 동물, 곤충 쪽의 성분은 강하게, 기계, 컴퓨터 쪽의 성분을 약하게 한다는 것인가?  

세상에는 언어가 너무나도 많고 그 언어에도 단어가 너무나도 많아서 그 단어들을 일일히 정하는 것은 불가능하다.  
그리고 컴퓨터를 위한 자연어-벡터 사전을 만든다 해도, 컴퓨터는 언어를 배웠다고 할 수 없다.  
영한사전을 통째로 외웠다 해서 영어를 잘하는 것이 아니듯 말이다.  

언어는, 작게 말해 단어는 그 단어가 얼마나, 어떻게, 어떤 단어와 같이 쓰이는지에 따라 의미가 있다.  
컴퓨터에게 실제 그 단어의 의미를 가르치진 못해도 단어를 수치적으로 나타낼 수 있는 것이다.  
우리의 언어는 사전 속에서 있는 것이 아니라 실제 사용되면서의미가 발현되기 때문에,  
실제 용례를 통계적인 자료로 만들어 학습을 시킨다.  

### 어떤 단어가 많이 쓰였는가

예를 들어 자연어 처리를 공부하는 책에서는 '자연어'나 '인공지능'이란 단어가 많이 쓰일 것이다.  
이 정보로 그 책은 '자연어', '인공지능'이 주제일 것이라고 예측을 할 수가 있고,  
다른 책들과 비교했을 때도 그 차이가 날 것이므로  
이를 통해서 책의 종류를 나눌 수 있을 것이다.   

### 단어가 어떤 순서로 쓰였는가

'커피' 다음에는 '마신다'가 높은 확률로 같이 쓰인다. '쏟았다', '먹었다' 도 올 수 있겠지만  
'마신다'의 확률이 훨씬 더 높을 것이다.   
이 확률을 알려주면 컴퓨터도 커피가 뭔지 마신다가 뭐하는 건진 몰라도  
적어도 '커피를 ???' 의 '???'에 '마신다' 가 올 것이라는 것을 예측할 수 있게 된다.  

### 어떤 단어가 같이 쓰였는가

순서도 순서이지만, 단어는 항상 붙어다니는 친구들이 있다.  
바늘 가는데 실 가듯이, 인공지능에 대해 설명하는 문서에는 항상 딥러닝이나 머신러닝이라는 단어가 나타날 것이다.  
은행 관련 문서에는 돈이나 투자, 이자 같은 단어가 나타날 것이다.  
이런 식으로 단어를 학습하면 항상 같은 순서로 나타나진 않아도 관련 있는 단어를 찾아내기 쉬울 것이다.  


이렇듯 단어를 임베딩하는 방법에는 크게 세 가지가 있다.  

1. 단어를 세기 (Bag of words, TF-IDF...)    
2. 단어의 순서 고려 (n-gram...)  
3. 같이 쓰인 단어 고려 (word2vec, fasttext...)    

## 1. 단어를 세기(count)  

문장 혹은 말뭉치(corpus) 들을 컴퓨터에게 주고 그 문장에 등장하는 단어를 세는 것이다.  

13인의아해가도로로질주하오.  
(길은막다른골목이적당하오.)  

제1의아해가무섭다고그리오.  
제2의아해도무섭다고그리오.  
제3의아해도무섭다고그리오. 
제4의아해도무섭다고그리오.  
...  
제13의아해도무섭다고그리오.  
13인의아해는무서운아해와무서워하는아해와그렇게뿐이모였소.(다른사정은없는것이차라리나았소)  

그중에1인의아해가무서운아해라도좋소.  
그중에2인의아해가무서운아해라도좋소.  
그중에2인의아해가무서워하는아해라도좋소.  
그중에1인의아해가무서워하는아해라도좋소.  

(길은뚫린골목이라도적당하오.)  
13인의아해가도로로질주하지아니하여도좋소.  

이상의 오감도라는 시가 있다.  
이 시에서 각 단어들을 세어보자.   

|1|2|...|11|12|13|제|아해|무섭다고|그리오|무서운|무서워하는|...|좋소|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|2|2|...|1|1|4|13|20|13|13|3|2|...|5|  

이렇게 단어를 단어의 등장 순서와 관계 없이 그 단어가 등장한 횟수(빈도)를 나타내는 것을  
백 오브 워즈(Bag of words)이라고 한다.  
이렇게 나타낸 행렬로 단어를 임베딩을 하는 것이다.  
마치 가방 안에 단어들이 들어가서 순서에 상관 없이 빈도만 체크한다 하여 Bag of words 라고 불린다.  

이렇게 나타내면, 문학도가 아니더라도 이 시에서 중요한 의미를 가지는 단어로  
'아해', '13', '무섭다' 의 단어가 있다는 사실을 유추해낼 수 있다.  
시가 아닌 어떤 설명글이라고 한다면, 내용은 이해 못해도 많이 나오는 단어로 그 주제 혹은 주제어를 파악할 수 있다.  

이 BoW 를 여러개를 묶어보도록 하자.  

여기 네 문장이 있다.  

문장 1 : 나는 쇼팽의 음악을 듣습니다.  
문장 2 : 어제 음악 시간에 잤어요.  
문장 3 : 나는 음악 듣는 것을 좋아합니다.  
문장 4 : 음악은 역시 클래식 음악.  

이 문장들의 BoW를 표로 나타내면 다음과 같다.   


|단어문장|문장 1|문장 2|문장 3|문장 4|  
|:---:|:---:|:---:|:---:|:---:|  
|나는|1|0|1|0|  
|쇼팽|1|0|0|0|  
|어제|0|1|0|0|  
|음악|1|1|1|2|  
|을(를)|1|0|1|0|
|은|0|0|0|1|
|역시|0|0|0|1|
|클래식|0|0|0|1|
|시간|0|1|0|0|  
|잤어요|0|1|0|0|  
|듣습니다|1|0|0|0|  
|듣는|0|0|1|0|  
|것을|0|0|1|0|  
|좋아합니다|0|0|1|0|  

이를 단어-문서 행렬(Term-Document Matrix) 라고 한다.  
이 행렬을 통해 문서 간의 유사도를 알 수 있다.  
(혹은 Document-Term-Matrix 이나 지금은 단어가 행이기 때문에 단어-문서 행렬이라 부르겠다.)  

이 행렬은 단어의 수 V, 문서의 수 D 로 V x D 차원의 행렬이다.  
이때 음악이라는 단어의 3행을 보자(위부터 0행으로 카운트한다.)  
3행은 `[1, 1, 1, 2]` 로, 다른 행들보다 값이 크다.  

이 사실로 네 문장이 음악이라는 주제를 공유하며, 특히 4번째 문장에서 음악이라는 단어가 중요하게 쓰이는 것을 알 수 있다.  
행렬의 각 값들은 TF (Term-Frequency) 라고 한다. 단어를 뜻하는 term 과 빈도를 나타내는 frequency 의 합성어이다.  

문서 간의 유사도를 점검하려면 TF 만으로는 부족하지 싶다.  
어떤 단어가 몇 개의 문장에서 나타났는지 구하면, 문장 간의 유사도를 더 정확히 알 수 있다.  
이를 DF 라고 한다. (Document-Frequency)  
예를 들어 음악이라는 단어는 네 문장에서 다 나타났으므로 4라고 할 수 있다.  

이 TDM 의 값인 TF 와 DF 를 곱하면, 여러 문서에 등장한 단어에 더 가중치를 둘 수 있다. (주: TDM 과 DF 의 행렬 곱셈이 아닌 원소별 곱셈이다.)  

그러나 만일 어떤 단어가 모든 문서에서 많이 등장한다면, 이를 특별한 단어라고 볼 수 없다.  
예를 들어 소설책들을 분석한다면, 1인칭 시점인 소설에서 자주 등장하는 '나는' 이라는 단어는  
문장과 소설을 해석하는데 크게 중요한 단어가 아니다.  

그래서 그냥 DF를 곱하는 것이 아니라, 전체 문장 수를 DF 만큼 나눈 만큼 곱하기로 한다.  
이때 그냥 DF를 가져가면 값이 매우 커지므로 log 을 취해주기로 하자.  
이를 IDF (Inversion-Document Frequency) 라고 한다.  
단어를 t, 전체 문장 수를 N 이라할 때 어떤 단어 t의 IDF 는 log(N/(1+df(t)) 이다.  
0번 등장하는 경우를 고려해 분모에 1을 더해주었다.  
이 IDF를 TF에 곱한 행렬은 다음과 같은 코드로 구한다.  


```python
import numpy as np
from math import log

tdm = {
    '나는      ' : np.array([1,0,1,0]),
    '쇼팽      ' : np.array([1,0,0,0]),
    '어제      ' : np.array([0,1,0,0]),
    '음악      ' : np.array([1,1,1,2]),
    '을(를)    ' : np.array([1,0,1,0]),
    '은        ' : np.array([0,0,0,1]),
    '역시      ' : np.array([0,0,0,1]),
    '클래식    ' : np.array([0,0,0,1]),
    '시간      ' : np.array([0,1,0,0]),
    '잤어요    ' : np.array([0,1,0,0]),
    '듣는      ' : np.array([0,0,1,0]),
    '것을      ' : np.array([0,0,1,0]),
    '좋아합니다 ' : np.array([0,0,1,0])
}

dm = np.array(np.sum([tf > 0 for tf in tdm.values()], axis=1))

tf_idf = {}
for i, (key, value) in enumerate(tdm.items()):
    tf_idf[key] = value * log(len(tdm)/1+dm[i])

for (key, value) in tf_idf.items():
    print("%-5s" % key, value)
```

format 으로 출력해도 한글이라 가지런히 정리가 안 돼서 임의로 공백을 넣어줬다...  
위 코드의 출력은 이러하다.  

![image](https://user-images.githubusercontent.com/22045424/74085682-00273580-4abf-11ea-9e2b-b3ae92d8c88c.png)

